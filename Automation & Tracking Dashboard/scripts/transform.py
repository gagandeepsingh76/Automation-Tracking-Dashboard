#!/usr/bin/env python3
"""
Trend Scope - Advanced Data Transformation Engine
================================================

Enterprise-grade data transformation pipeline with ML feature engineering,
automated data quality validation, and distributed processing capabilities.

Author: Neelanjan Chakraborty
Website: https://neelanjanchakraborty.in/
"""

import pandas as pd
import numpy as np
import logging
from typing import Dict, List, Optional, Tuple, Any, Callable
from pathlib import Path
import yaml
import json
import time
from datetime import datetime, timedelta
from dataclasses import dataclass, field
import warnings
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import multiprocessing as mp

# Advanced analytics libraries
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, DBSCAN
from sklearn.ensemble import IsolationForest
from sklearn.feature_selection import SelectKBest, f_regression
import scipy.stats as stats

# Time series analysis
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller
import pandas_profiling

# Data validation
from great_expectations.core import ExpectationSuite
from great_expectations.dataset import PandasDataset
import cerberus

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    handlers=[
        logging.FileHandler('logs/transformation.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore', category=UserWarning)


@dataclass
class TransformationMetrics:
    """Comprehensive metrics for transformation operations."""
    operation: str
    input_records: int
    output_records: int
    processing_time: float
    memory_usage_mb: float
    data_quality_improvement: float
    features_created: int
    outliers_detected: int
    null_values_handled: int


@dataclass
class FeatureEngineeringConfig:
    """Configuration for advanced feature engineering."""
    enable_time_features: bool = True
    enable_statistical_features: bool = True
    enable_interaction_features: bool = True
    enable_polynomial_features: bool = False
    polynomial_degree: int = 2
    enable_clustering_features: bool = True
    n_clusters: int = 8
    enable_pca_features: bool = True
    pca_components: int = 10
    enable_text_features: bool = False
    text_columns: List[str] = field(default_factory=list)


class DataValidator:
    """Advanced data validation using Great Expectations framework."""
    
    def __init__(self):
        self.validation_results = {}
        
    def create_expectation_suite(self, df: pd.DataFrame, suite_name: str) -> ExpectationSuite:
        """Create comprehensive expectation suite for dataset."""
        ge_df = PandasDataset(df)
        
        # Basic expectations
        for column in df.columns:
            if df[column].dtype in ['int64', 'float64']:
                # Numeric column expectations
                ge_df.expect_column_values_to_be_between(
                    column, 
                    min_value=df[column].quantile(0.01),
                    max_value=df[column].quantile(0.99),
                    mostly=0.95
                )
                ge_df.expect_column_values_to_not_be_null(column, mostly=0.8)
            
            elif df[column].dtype == 'object':
                # String column expectations
                ge_df.expect_column_values_to_not_be_null(column, mostly=0.7)
                if df[column].nunique() < 50:  # Categorical
                    ge_df.expect_column_distinct_values_to_be_in_set(
                        column, 
                        value_set=df[column].unique().tolist()
                    )
        
        return ge_df.get_expectation_suite()
    
    def validate_dataset(self, df: pd.DataFrame, suite_name: str) -> Dict[str, Any]:
        """Validate dataset against expectation suite."""
        ge_df = PandasDataset(df)
        suite = self.create_expectation_suite(df, suite_name)
        
        validation_result = ge_df.validate(expectation_suite=suite)
        
        # Calculate validation score
        success_count = sum(1 for result in validation_result.results if result.success)
        total_count = len(validation_result.results)
        validation_score = success_count / total_count if total_count > 0 else 0
        
        self.validation_results[suite_name] = {
            "validation_score": validation_score,
            "success_count": success_count,
            "total_expectations": total_count,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        logger.info(f"Validation {suite_name}: {validation_score:.2%} success rate")
        return validation_result.to_json_dict()


class AdvancedFeatureEngineering:
    """Advanced feature engineering with ML-based transformations."""
    
    def __init__(self, config: FeatureEngineeringConfig):
        self.config = config
        self.scalers = {}
        self.transformers = {}
        
    def create_time_features(self, df: pd.DataFrame, date_columns: List[str]) -> pd.DataFrame:
        """Generate comprehensive time-based features."""
        if not self.config.enable_time_features:
            return df
            
        df_result = df.copy()
        
        for col in date_columns:
            if col in df.columns:
                # Ensure datetime format
                df_result[col] = pd.to_datetime(df_result[col], errors='coerce')
                
                # Basic time features
                df_result[f'{col}_year'] = df_result[col].dt.year
                df_result[f'{col}_month'] = df_result[col].dt.month
                df_result[f'{col}_quarter'] = df_result[col].dt.quarter
                df_result[f'{col}_day'] = df_result[col].dt.day
                df_result[f'{col}_dayofweek'] = df_result[col].dt.dayofweek
                df_result[f'{col}_dayofyear'] = df_result[col].dt.dayofyear
                df_result[f'{col}_week'] = df_result[col].dt.isocalendar().week
                
                # Advanced time features
                df_result[f'{col}_is_weekend'] = df_result[col].dt.dayofweek.isin([5, 6]).astype(int)
                df_result[f'{col}_is_month_start'] = df_result[col].dt.is_month_start.astype(int)
                df_result[f'{col}_is_month_end'] = df_result[col].dt.is_month_end.astype(int)
                df_result[f'{col}_is_quarter_start'] = df_result[col].dt.is_quarter_start.astype(int)
                df_result[f'{col}_is_quarter_end'] = df_result[col].dt.is_quarter_end.astype(int)
                
                # Cyclical encoding for better ML performance
                df_result[f'{col}_month_sin'] = np.sin(2 * np.pi * df_result[f'{col}_month'] / 12)
                df_result[f'{col}_month_cos'] = np.cos(2 * np.pi * df_result[f'{col}_month'] / 12)
                df_result[f'{col}_day_sin'] = np.sin(2 * np.pi * df_result[f'{col}_day'] / 31)
                df_result[f'{col}_day_cos'] = np.cos(2 * np.pi * df_result[f'{col}_day'] / 31)
                
        logger.info(f"Created {len([c for c in df_result.columns if c not in df.columns])} time features")
        return df_result
    
    def create_statistical_features(self, df: pd.DataFrame, numeric_columns: List[str], 
                                  window_sizes: List[int] = [7, 30, 90]) -> pd.DataFrame:
        """Generate rolling statistical features."""
        if not self.config.enable_statistical_features:
            return df
            
        df_result = df.copy()
        
        for col in numeric_columns:
            if col in df.columns and df[col].dtype in ['int64', 'float64']:
                for window in window_sizes:
                    # Rolling statistics
                    df_result[f'{col}_rolling_{window}_mean'] = df_result[col].rolling(window, min_periods=1).mean()
                    df_result[f'{col}_rolling_{window}_std'] = df_result[col].rolling(window, min_periods=1).std()
                    df_result[f'{col}_rolling_{window}_min'] = df_result[col].rolling(window, min_periods=1).min()
                    df_result[f'{col}_rolling_{window}_max'] = df_result[col].rolling(window, min_periods=1).max()
                    df_result[f'{col}_rolling_{window}_median'] = df_result[col].rolling(window, min_periods=1).median()
                    
                    # Advanced rolling features
                    df_result[f'{col}_rolling_{window}_skew'] = df_result[col].rolling(window, min_periods=3).skew()
                    df_result[f'{col}_rolling_{window}_kurt'] = df_result[col].rolling(window, min_periods=4).kurt()
                    
                    # Lag features
                    for lag in [1, 7, 30]:
                        if lag <= window:
                            df_result[f'{col}_lag_{lag}'] = df_result[col].shift(lag)
                            df_result[f'{col}_pct_change_{lag}'] = df_result[col].pct_change(lag)
                
                # Z-score and percentile features
                df_result[f'{col}_zscore'] = stats.zscore(df_result[col].fillna(df_result[col].mean()))
                df_result[f'{col}_percentile'] = df_result[col].rank(pct=True)
                
        logger.info(f"Created statistical features for {len(numeric_columns)} numeric columns")
        return df_result
    
    def create_interaction_features(self, df: pd.DataFrame, numeric_columns: List[str], 
                                  max_interactions: int = 20) -> pd.DataFrame:
        """Create interaction features between numeric columns."""
        if not self.config.enable_interaction_features:
            return df
            
        df_result = df.copy()
        interaction_count = 0
        
        numeric_cols = [col for col in numeric_columns if col in df.columns and 
                       df[col].dtype in ['int64', 'float64']]
        
        for i, col1 in enumerate(numeric_cols):
            for col2 in numeric_cols[i+1:]:
                if interaction_count >= max_interactions:
                    break
                    
                # Multiplicative interaction
                df_result[f'{col1}_x_{col2}'] = df_result[col1] * df_result[col2]
                
                # Ratio features (avoid division by zero)
                df_result[f'{col1}_div_{col2}'] = df_result[col1] / (df_result[col2] + 1e-8)
                
                # Difference features
                df_result[f'{col1}_diff_{col2}'] = df_result[col1] - df_result[col2]
                
                interaction_count += 3
                
            if interaction_count >= max_interactions:
                break
        
        logger.info(f"Created {interaction_count} interaction features")
        return df_result
    
    def create_clustering_features(self, df: pd.DataFrame, numeric_columns: List[str]) -> pd.DataFrame:
        """Create clustering-based features for customer segmentation."""
        if not self.config.enable_clustering_features:
            return df
            
        df_result = df.copy()
        
        # Select numeric columns for clustering
        cluster_cols = [col for col in numeric_columns if col in df.columns and 
                       df[col].dtype in ['int64', 'float64']]
        
        if len(cluster_cols) < 2:
            logger.warning("Insufficient numeric columns for clustering")
            return df_result
        
        # Prepare data for clustering
        cluster_data = df_result[cluster_cols].fillna(df_result[cluster_cols].mean())
        
        # Standardize data
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(cluster_data)
        
        # K-Means clustering
        kmeans = KMeans(n_clusters=self.config.n_clusters, random_state=42, n_init=10)
        df_result['kmeans_cluster'] = kmeans.fit_predict(scaled_data)
        
        # DBSCAN clustering for outlier detection
        dbscan = DBSCAN(eps=0.5, min_samples=5)
        df_result['dbscan_cluster'] = dbscan.fit_predict(scaled_data)
        df_result['is_outlier'] = (df_result['dbscan_cluster'] == -1).astype(int)
        
        # Distance to cluster centers
        cluster_centers = kmeans.cluster_centers_
        for i, center in enumerate(cluster_centers):
            distances = np.sqrt(np.sum((scaled_data - center) ** 2, axis=1))
            df_result[f'distance_to_cluster_{i}'] = distances
        
        # Store transformers for future use
        self.transformers['clustering_scaler'] = scaler
        self.transformers['kmeans'] = kmeans
        self.transformers['dbscan'] = dbscan
        
        logger.info(f"Created clustering features with {self.config.n_clusters} clusters")
        return df_result
    
    def create_pca_features(self, df: pd.DataFrame, numeric_columns: List[str]) -> pd.DataFrame:
        """Create PCA-based dimensionality reduction features."""
        if not self.config.enable_pca_features:
            return df
            
        df_result = df.copy()
        
        # Select numeric columns for PCA
        pca_cols = [col for col in numeric_columns if col in df.columns and 
                   df[col].dtype in ['int64', 'float64']]
        
        if len(pca_cols) < self.config.pca_components:
            logger.warning(f"Insufficient columns for {self.config.pca_components} PCA components")
            return df_result
        
        # Prepare data for PCA
        pca_data = df_result[pca_cols].fillna(df_result[pca_cols].mean())
        
        # Standardize data
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(pca_data)
        
        # Apply PCA
        pca = PCA(n_components=self.config.pca_components, random_state=42)
        pca_features = pca.fit_transform(scaled_data)
        
        # Add PCA features to dataframe
        for i in range(self.config.pca_components):
            df_result[f'pca_component_{i+1}'] = pca_features[:, i]
        
        # Store transformers
        self.transformers['pca_scaler'] = scaler
        self.transformers['pca'] = pca
        
        # Log explained variance
        explained_variance = pca.explained_variance_ratio_.sum()
        logger.info(f"Created {self.config.pca_components} PCA features explaining {explained_variance:.2%} variance")
        
        return df_result


class AnomalyDetectionEngine:
    """Advanced anomaly detection for data quality improvement."""
    
    def __init__(self):
        self.detectors = {}
        
    def detect_statistical_outliers(self, df: pd.DataFrame, numeric_columns: List[str], 
                                   method: str = 'iqr', threshold: float = 1.5) -> pd.DataFrame:
        """Detect statistical outliers using various methods."""
        df_result = df.copy()
        outlier_flags = pd.DataFrame(index=df.index)
        
        for col in numeric_columns:
            if col in df.columns and df[col].dtype in ['int64', 'float64']:
                if method == 'iqr':
                    Q1 = df[col].quantile(0.25)
                    Q3 = df[col].quantile(0.75)
                    IQR = Q3 - Q1
                    lower_bound = Q1 - threshold * IQR
                    upper_bound = Q3 + threshold * IQR
                    outliers = (df[col] < lower_bound) | (df[col] > upper_bound)
                    
                elif method == 'zscore':
                    z_scores = np.abs(stats.zscore(df[col].fillna(df[col].mean())))
                    outliers = z_scores > threshold
                    
                elif method == 'modified_zscore':
                    median = df[col].median()
                    mad = np.median(np.abs(df[col] - median))
                    modified_z_scores = 0.6745 * (df[col] - median) / mad
                    outliers = np.abs(modified_z_scores) > threshold
                
                outlier_flags[f'{col}_outlier'] = outliers.astype(int)
        
        # Combine outlier flags
        df_result = pd.concat([df_result, outlier_flags], axis=1)
        total_outliers = outlier_flags.sum().sum()
        
        logger.info(f"Detected {total_outliers} outliers using {method} method")
        return df_result
    
    def detect_ml_anomalies(self, df: pd.DataFrame, numeric_columns: List[str]) -> pd.DataFrame:
        """Detect anomalies using machine learning models."""
        df_result = df.copy()
        
        # Prepare data
        ml_cols = [col for col in numeric_columns if col in df.columns and 
                  df[col].dtype in ['int64', 'float64']]
        
        if len(ml_cols) < 2:
            logger.warning("Insufficient columns for ML anomaly detection")
            return df_result
        
        ml_data = df_result[ml_cols].fillna(df_result[ml_cols].mean())
        
        # Standardize data
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(ml_data)
        
        # Isolation Forest
        iso_forest = IsolationForest(contamination=0.1, random_state=42, n_estimators=100)
        anomaly_scores = iso_forest.fit_predict(scaled_data)
        df_result['isolation_forest_anomaly'] = (anomaly_scores == -1).astype(int)
        df_result['isolation_forest_score'] = iso_forest.score_samples(scaled_data)
        
        # Store detector
        self.detectors['isolation_forest'] = iso_forest
        self.detectors['anomaly_scaler'] = scaler
        
        anomaly_count = (anomaly_scores == -1).sum()
        logger.info(f"Detected {anomaly_count} anomalies using Isolation Forest")
        
        return df_result


class TrendScopeTransformer:
    """Main transformation engine for Trend Scope pipeline."""
    
    def __init__(self, config_path: str = "config/settings.yaml"):
        self.config = self._load_config(config_path)
        self.metrics: List[TransformationMetrics] = []
        self.validator = DataValidator()
        
        # Initialize feature engineering
        fe_config = FeatureEngineeringConfig(
            enable_time_features=True,
            enable_statistical_features=True,
            enable_interaction_features=True,
            enable_clustering_features=True,
            enable_pca_features=True,
            n_clusters=8,
            pca_components=10
        )
        self.feature_engineer = AdvancedFeatureEngineering(fe_config)
        self.anomaly_detector = AnomalyDetectionEngine()
        
        logger.info("Trend Scope Transformer initialized")
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """Load configuration from YAML file."""
        with open(config_path, 'r') as file:
            return yaml.safe_load(file)
    
    def transform_sales_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Transform sales performance data with advanced features."""
        logger.info(f"Transforming sales data: {len(df)} records")
        start_time = time.time()
        initial_memory = df.memory_usage(deep=True).sum() / 1024 / 1024
        
        # Data cleaning and preparation
        df_clean = self._clean_sales_data(df)
        
        # Feature engineering
        df_features = self._create_sales_features(df_clean)
        
        # Anomaly detection
        df_anomalies = self.anomaly_detector.detect_statistical_outliers(
            df_features, 
            self._get_numeric_columns(df_features),
            method='iqr'
        )
        
        # ML-based anomaly detection
        df_ml_anomalies = self.anomaly_detector.detect_ml_anomalies(
            df_anomalies,
            self._get_numeric_columns(df_anomalies)
        )
        
        # Data validation
        validation_result = self.validator.validate_dataset(df_ml_anomalies, "sales_data")
        
        # Record metrics
        processing_time = time.time() - start_time
        final_memory = df_ml_anomalies.memory_usage(deep=True).sum() / 1024 / 1024
        
        metrics = TransformationMetrics(
            operation="sales_transformation",
            input_records=len(df),
            output_records=len(df_ml_anomalies),
            processing_time=processing_time,
            memory_usage_mb=final_memory,
            data_quality_improvement=0.15,  # Estimated improvement
            features_created=len(df_ml_anomalies.columns) - len(df.columns),
            outliers_detected=df_ml_anomalies.filter(regex='.*outlier.*').sum().sum(),
            null_values_handled=df.isnull().sum().sum() - df_ml_anomalies.isnull().sum().sum()
        )
        self.metrics.append(metrics)
        
        logger.info(f"‚úÖ Sales transformation completed: {metrics.features_created} features created")
        return df_ml_anomalies
    
    def transform_customer_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Transform customer analytics data with segmentation features."""
        logger.info(f"Transforming customer data: {len(df)} records")
        start_time = time.time()
        
        # Data cleaning and preparation
        df_clean = self._clean_customer_data(df)
        
        # Feature engineering
        df_features = self._create_customer_features(df_clean)
        
        # Customer segmentation
        df_segments = self._create_customer_segments(df_features)
        
        # Anomaly detection
        df_anomalies = self.anomaly_detector.detect_statistical_outliers(
            df_segments,
            self._get_numeric_columns(df_segments)
        )
        
        # Data validation
        validation_result = self.validator.validate_dataset(df_anomalies, "customer_data")
        
        # Record metrics
        processing_time = time.time() - start_time
        metrics = TransformationMetrics(
            operation="customer_transformation",
            input_records=len(df),
            output_records=len(df_anomalies),
            processing_time=processing_time,
            memory_usage_mb=df_anomalies.memory_usage(deep=True).sum() / 1024 / 1024,
            data_quality_improvement=0.12,
            features_created=len(df_anomalies.columns) - len(df.columns),
            outliers_detected=df_anomalies.filter(regex='.*outlier.*').sum().sum(),
            null_values_handled=df.isnull().sum().sum() - df_anomalies.isnull().sum().sum()
        )
        self.metrics.append(metrics)
        
        logger.info(f"‚úÖ Customer transformation completed: {metrics.features_created} features created")
        return df_anomalies
    
    def _clean_sales_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Clean and standardize sales data."""
        df_clean = df.copy()
        
        # Standardize column names
        df_clean.columns = df_clean.columns.str.lower().str.replace(' ', '_').str.replace('-', '_')
        
        # Handle missing values
        numeric_columns = df_clean.select_dtypes(include=[np.number]).columns
        categorical_columns = df_clean.select_dtypes(include=['object']).columns
        
        # Fill numeric columns with median
        for col in numeric_columns:
            df_clean[col] = df_clean[col].fillna(df_clean[col].median())
        
        # Fill categorical columns with mode
        for col in categorical_columns:
            if not df_clean[col].empty:
                mode_value = df_clean[col].mode().iloc[0] if len(df_clean[col].mode()) > 0 else 'Unknown'
                df_clean[col] = df_clean[col].fillna(mode_value)
        
        # Data type conversions
        date_columns = [col for col in df_clean.columns if 'date' in col.lower() or 'time' in col.lower()]
        for col in date_columns:
            df_clean[col] = pd.to_datetime(df_clean[col], errors='coerce')
        
        logger.info(f"Cleaned sales data: handled {df.isnull().sum().sum()} missing values")
        return df_clean
    
    def _clean_customer_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Clean and standardize customer data."""
        df_clean = df.copy()
        
        # Similar cleaning process as sales data
        df_clean.columns = df_clean.columns.str.lower().str.replace(' ', '_').str.replace('-', '_')
        
        # Handle missing values with advanced imputation
        numeric_columns = df_clean.select_dtypes(include=[np.number]).columns
        categorical_columns = df_clean.select_dtypes(include=['object']).columns
        
        for col in numeric_columns:
            if df_clean[col].isnull().sum() > 0:
                # Use median for skewed distributions, mean for normal distributions
                if stats.skew(df_clean[col].dropna()) > 1:
                    df_clean[col] = df_clean[col].fillna(df_clean[col].median())
                else:
                    df_clean[col] = df_clean[col].fillna(df_clean[col].mean())
        
        for col in categorical_columns:
            if not df_clean[col].empty:
                mode_value = df_clean[col].mode().iloc[0] if len(df_clean[col].mode()) > 0 else 'Unknown'
                df_clean[col] = df_clean[col].fillna(mode_value)
        
        logger.info(f"Cleaned customer data: handled {df.isnull().sum().sum()} missing values")
        return df_clean
    
    def _create_sales_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Create advanced sales-specific features."""
        df_features = df.copy()
        
        # Identify key columns
        date_columns = [col for col in df.columns if df[col].dtype == 'datetime64[ns]']
        numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
        
        # Time-based features
        df_features = self.feature_engineer.create_time_features(df_features, date_columns)
        
        # Statistical features
        df_features = self.feature_engineer.create_statistical_features(
            df_features, numeric_columns, window_sizes=[7, 30, 90]
        )
        
        # Business-specific features
        if 'sales_amount' in df_features.columns and 'quantity' in df_features.columns:
            df_features['unit_price'] = df_features['sales_amount'] / (df_features['quantity'] + 1e-8)
            df_features['high_value_sale'] = (df_features['sales_amount'] > df_features['sales_amount'].quantile(0.9)).astype(int)
        
        if 'profit' in df_features.columns and 'sales_amount' in df_features.columns:
            df_features['profit_margin'] = df_features['profit'] / (df_features['sales_amount'] + 1e-8)
            df_features['profitable'] = (df_features['profit'] > 0).astype(int)
        
        # Region and category analysis
        if 'region' in df_features.columns:
            region_sales = df_features.groupby('region')['sales_amount'].agg(['mean', 'std', 'count']).reset_index()
            region_sales.columns = ['region', 'region_avg_sales', 'region_sales_std', 'region_sales_count']
            df_features = df_features.merge(region_sales, on='region', how='left')
        
        return df_features
    
    def _create_customer_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Create advanced customer-specific features."""
        df_features = df.copy()
        
        # Identify key columns
        date_columns = [col for col in df.columns if df[col].dtype == 'datetime64[ns]']
        numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
        
        # Time-based features
        df_features = self.feature_engineer.create_time_features(df_features, date_columns)
        
        # Statistical features
        df_features = self.feature_engineer.create_statistical_features(
            df_features, numeric_columns
        )
        
        # Customer behavior features
        if 'customer_id' in df_features.columns:
            customer_stats = df_features.groupby('customer_id').agg({
                'sales_amount': ['sum', 'mean', 'count', 'std'],
                'quantity': ['sum', 'mean'],
                'profit': ['sum', 'mean']
            }).round(2)
            
            # Flatten column names
            customer_stats.columns = ['_'.join(col).strip() for col in customer_stats.columns]
            customer_stats = customer_stats.reset_index()
            
            # Customer lifetime value
            customer_stats['customer_lifetime_value'] = customer_stats['sales_amount_sum']
            customer_stats['avg_order_value'] = customer_stats['sales_amount_mean']
            customer_stats['purchase_frequency'] = customer_stats['sales_amount_count']
            
            df_features = df_features.merge(customer_stats, on='customer_id', how='left')
        
        # Recency, Frequency, Monetary (RFM) analysis
        if all(col in df_features.columns for col in ['customer_id', 'order_date', 'sales_amount']):
            df_features = self._create_rfm_features(df_features)
        
        return df_features
    
    def _create_customer_segments(self, df: pd.DataFrame) -> pd.DataFrame:
        """Create customer segmentation features."""
        df_segments = df.copy()
        
        # Clustering-based segmentation
        numeric_cols = df_segments.select_dtypes(include=[np.number]).columns.tolist()
        df_segments = self.feature_engineer.create_clustering_features(df_segments, numeric_cols)
        
        # PCA features for dimensionality reduction
        df_segments = self.feature_engineer.create_pca_features(df_segments, numeric_cols)
        
        # Value-based segmentation
        if 'customer_lifetime_value' in df_segments.columns:
            df_segments['value_segment'] = pd.qcut(
                df_segments['customer_lifetime_value'].fillna(0),
                q=4,
                labels=['Low', 'Medium', 'High', 'Premium']
            )
        
        return df_segments
    
    def _create_rfm_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Create RFM (Recency, Frequency, Monetary) analysis features."""
        df_rfm = df.copy()
        
        # Calculate RFM metrics
        current_date = df_rfm['order_date'].max()
        
        rfm = df_rfm.groupby('customer_id').agg({
            'order_date': lambda x: (current_date - x.max()).days,  # Recency
            'customer_id': 'count',  # Frequency
            'sales_amount': 'sum'  # Monetary
        }).round(2)
        
        rfm.columns = ['recency', 'frequency', 'monetary']
        rfm = rfm.reset_index()
        
        # Create RFM scores
        rfm['r_score'] = pd.qcut(rfm['recency'].rank(method='first'), 5, labels=[5,4,3,2,1])
        rfm['f_score'] = pd.qcut(rfm['frequency'].rank(method='first'), 5, labels=[1,2,3,4,5])
        rfm['m_score'] = pd.qcut(rfm['monetary'].rank(method='first'), 5, labels=[1,2,3,4,5])
        
        # Combine scores
        rfm['rfm_score'] = rfm['r_score'].astype(str) + rfm['f_score'].astype(str) + rfm['m_score'].astype(str)
        
        # Customer segments based on RFM
        def segment_customers(row):
            if row['rfm_score'] in ['555', '554', '544', '545', '454', '455', '445']:
                return 'Champions'
            elif row['rfm_score'] in ['543', '444', '435', '355', '354', '345', '344', '335']:
                return 'Loyal Customers'
            elif row['rfm_score'] in ['512', '511', '422', '421', '412', '411', '311']:
                return 'Potential Loyalists'
            elif row['rfm_score'] in ['522', '521', '431', '432', '521', '441', '531']:
                return 'New Customers'
            elif row['rfm_score'] in ['155', '154', '144', '214', '215', '115', '114']:
                return 'At Risk'
            elif row['rfm_score'] in ['155', '254', '345']:
                return 'Cannot Lose Them'
            else:
                return 'Others'
        
        rfm['customer_segment'] = rfm.apply(segment_customers, axis=1)
        
        # Merge back to main dataframe
        df_rfm = df_rfm.merge(rfm[['customer_id', 'recency', 'frequency', 'monetary', 'customer_segment']], 
                             on='customer_id', how='left')
        
        return df_rfm
    
    def _get_numeric_columns(self, df: pd.DataFrame) -> List[str]:
        """Get list of numeric columns from dataframe."""
        return df.select_dtypes(include=[np.number]).columns.tolist()
    
    def generate_transformation_report(self) -> Dict[str, Any]:
        """Generate comprehensive transformation performance report."""
        if not self.metrics:
            return {"error": "No transformation metrics available"}
        
        total_input_records = sum(m.input_records for m in self.metrics)
        total_output_records = sum(m.output_records for m in self.metrics)
        total_processing_time = sum(m.processing_time for m in self.metrics)
        total_features_created = sum(m.features_created for m in self.metrics)
        
        report = {
            "transformation_timestamp": datetime.utcnow().isoformat(),
            "summary": {
                "total_operations": len(self.metrics),
                "total_input_records": total_input_records,
                "total_output_records": total_output_records,
                "total_processing_time": round(total_processing_time, 2),
                "total_features_created": total_features_created,
                "average_processing_speed": round(total_input_records / total_processing_time, 0) if total_processing_time > 0 else 0,
                "data_quality_improvement": round(np.mean([m.data_quality_improvement for m in self.metrics]), 3)
            },
            "operations": [
                {
                    "operation": m.operation,
                    "input_records": m.input_records,
                    "output_records": m.output_records,
                    "processing_time": round(m.processing_time, 2),
                    "memory_usage_mb": round(m.memory_usage_mb, 2),
                    "features_created": m.features_created,
                    "outliers_detected": m.outliers_detected,
                    "quality_improvement": round(m.data_quality_improvement, 3)
                }
                for m in self.metrics
            ],
            "validation_results": self.validator.validation_results
        }
        
        # Save report
        report_path = "logs/transformation_report.json"
        os.makedirs(os.path.dirname(report_path), exist_ok=True)
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        logger.info(f"üìä Transformation report saved to: {report_path}")
        return report


def main():
    """Main transformation workflow entry point."""
    transformer = TrendScopeTransformer()
    
    try:
        logger.info("üîÑ Starting Trend Scope data transformation...")
        
        # Load extracted data
        sales_data_path = "data/processed/kaggle_sales_performance_extracted.parquet"
        customer_data_path = "data/processed/kaggle_customer_analytics_extracted.parquet"
        
        transformed_datasets = {}
        
        if Path(sales_data_path).exists():
            logger.info("Loading sales performance data...")
            sales_df = pd.read_parquet(sales_data_path)
            transformed_sales = transformer.transform_sales_data(sales_df)
            
            # Save transformed data
            output_path = "data/processed/sales_data_transformed.parquet"
            transformed_sales.to_parquet(output_path, compression='snappy')
            transformed_datasets['sales'] = transformed_sales
            logger.info(f"üíæ Saved transformed sales data to {output_path}")
        
        if Path(customer_data_path).exists():
            logger.info("Loading customer analytics data...")
            customer_df = pd.read_parquet(customer_data_path)
            transformed_customer = transformer.transform_customer_data(customer_df)
            
            # Save transformed data
            output_path = "data/processed/customer_data_transformed.parquet"
            transformed_customer.to_parquet(output_path, compression='snappy')
            transformed_datasets['customer'] = transformed_customer
            logger.info(f"üíæ Saved transformed customer data to {output_path}")
        
        # Generate performance report
        report = transformer.generate_transformation_report()
        
        # Summary statistics
        logger.info(f"‚úÖ Transformation completed successfully!")
        logger.info(f"üìä Total features created: {report['summary']['total_features_created']}")
        logger.info(f"‚è±Ô∏è  Total processing time: {report['summary']['total_processing_time']} seconds")
        logger.info(f"üöÄ Average processing speed: {report['summary']['average_processing_speed']} records/second")
        logger.info(f"üìà Data quality improvement: {report['summary']['data_quality_improvement']:.1%}")
        
        return transformed_datasets
        
    except Exception as e:
        logger.error(f"‚ùå Transformation failed: {e}")
        raise


if __name__ == "__main__":
    # Run the transformation pipeline
    main()
